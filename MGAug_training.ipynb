{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2cd1b6-69b3-41e0-83a7-a02f7b69fba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acc1db-7425-431e-b3f8-bcc830f43cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from typing import Dict, SupportsRound, Tuple, Any\n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch,gc\n",
    "from torch.autograd import grad\n",
    "from torch.autograd import Variable\n",
    "import torch.fft ############### Pytorch >= 1.8.0\n",
    "import torch.nn.functional as F\n",
    "import SimpleITK as sitk\n",
    "import os, glob\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "from PIL import Image\n",
    "import torch.nn.functional as nnf\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR,CosineAnnealingWarmRestarts,StepLR\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import random\n",
    "import yaml\n",
    "from tqdm import tqdm, trange\n",
    "from numpy import zeros, newaxis\n",
    "from easydict import EasyDict as edict\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import cv2\n",
    "import lagomorph as lm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda\"\n",
    "else:\n",
    "    dev = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21763858-5341-474c-b395-8a46bc6391cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################Parameter Loading#######################\n",
    "def read_yaml(path):\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            file = edict(yaml.load(f, Loader=yaml.FullLoader))\n",
    "        return file\n",
    "    except:\n",
    "        print('NO FILE READ!')\n",
    "        return None\n",
    "    \n",
    "para = read_yaml('./parameters.yml')\n",
    "\n",
    "xDim = para.data.x \n",
    "yDim = para.data.y\n",
    "zDim = para.data.z\n",
    "\n",
    "def loss_Reg(y_pred):\n",
    "        # For 3D reg\n",
    "        # dy = torch.abs(y_pred[:, :, 1:, :, :] - y_pred[:, :, :-1, :, :])\n",
    "        # dx = torch.abs(y_pred[:, :, :, 1:, :] - y_pred[:, :, :, :-1, :])\n",
    "        # dz = torch.abs(y_pred[:, :, :, :, 1:] - y_pred[:, :, :, :, :-1])\n",
    "        # dy = dy * dy\n",
    "        # dx = dx * dx\n",
    "        # dz = dz * dz\n",
    "        # d = torch.mean(dx) + torch.mean(dy) + torch.mean(dz)\n",
    "        # grad = d / 3.0\n",
    "\n",
    "        dy = torch.abs(y_pred[:, :, 1:, :] - y_pred[:, :, :-1, :])\n",
    "        dx = torch.abs(y_pred[:, :, :, 1:] - y_pred[:, :, :, :-1])\n",
    "\n",
    "        dy = dy * dy\n",
    "        dx = dx * dx\n",
    "        d = torch.mean(dx) + torch.mean(dy) \n",
    "        grad = d / 2.0\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd4511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the dataset\n",
    "\n",
    "# Load the NumPy array from the pickle file\n",
    "with open('./datasets/mnist_10p_train_x.pkl', 'rb') as file:\n",
    "    final_X_train = pickle.load(file)\n",
    "\n",
    "with open('./datasets/mnist_10p_train_y.pkl', 'rb') as file:\n",
    "    final_y_train = pickle.load(file)\n",
    "\n",
    "with open('./datasets/mnist_10p_test_x.pkl', 'rb') as file:\n",
    "    final_X_test = pickle.load(file)\n",
    "\n",
    "with open('./datasets/mnist_10p_test_y.pkl', 'rb') as file:\n",
    "    final_y_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d060104-6824-42ed-93b9-877032e29369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ##################Training Data Loading##########################\n",
    "# readfilename = './2DShape/train_data' + '.json'\n",
    "# datapath = './2DShape/'\n",
    "# data = json.load(open(readfilename, 'r'))\n",
    "# outputs = []\n",
    "# keyword = 'train'\n",
    "# # outputs = np.array(outputs)\n",
    "\n",
    "# for i in trange (0,len(data[keyword])):\n",
    "#     filename_src = datapath + data[keyword][i]['source']\n",
    "#     itkimage_src = sitk.ReadImage(filename_src)\n",
    "#     source_scan = sitk.GetArrayFromImage(itkimage_src)\n",
    "#     source_scan = cv2.resize(source_scan, (64, 64))\n",
    "#     if source_scan.ndim == 3 and source_scan.shape == (64, 64, 4):\n",
    "#         source_scan = source_scan[:,:,0]\n",
    "    \n",
    "#     filename_tar = datapath + data[keyword][i]['target']\n",
    "#     itkimage_tar = sitk.ReadImage(filename_tar)\n",
    "#     target_scan = sitk.GetArrayFromImage(itkimage_tar)\n",
    "#     target_scan = cv2.resize(target_scan, (64, 64))\n",
    "#     if target_scan.ndim == 3 and target_scan.shape == (64, 64, 4):\n",
    "#         target_scan = target_scan[:,:,0]\n",
    "        \n",
    "#     # print(source_scan.shape, target_scan.shape)\n",
    "    \n",
    "#     # print(i, source_scan.min(), source_scan.max(), target_scan.min(), target_scan.max())\n",
    "    \n",
    "#     source_scan = (source_scan - np.min(source_scan)) / (np.max(source_scan) - np.min(source_scan))\n",
    "#     target_scan = (target_scan - np.min(target_scan)) / (np.max(target_scan) - np.min(target_scan))\n",
    "    \n",
    "#     pair = np.concatenate((source_scan[newaxis,:], target_scan[newaxis,:]), axis=0)\n",
    "#     outputs.append(pair)\n",
    "#     # print(pair.shape)\n",
    "#     # print(i, source_scan.min(), source_scan.max(), target_scan.min(), target_scan.max())\n",
    "\n",
    "# train = torch.FloatTensor(outputs)\n",
    "# print (train.shape)\n",
    "\n",
    "# ##################Testing Data Loading##########################\n",
    "# readfilename = './2DShape/test_data' + '.json'\n",
    "# datapath = './2DShape/'\n",
    "# data = json.load(open(readfilename, 'r'))\n",
    "# outputs = []\n",
    "# keyword = 'test'\n",
    "# # outputs = np.array(outputs)\n",
    "\n",
    "# for i in trange (0,len(data[keyword])):\n",
    "#     filename_src = datapath + data[keyword][i]['source']\n",
    "#     itkimage_src = sitk.ReadImage(filename_src)\n",
    "#     source_scan = sitk.GetArrayFromImage(itkimage_src)\n",
    "#     source_scan = cv2.resize(source_scan, (64, 64))\n",
    "#     if source_scan.ndim == 3 and source_scan.shape == (64, 64, 4):\n",
    "#         source_scan = source_scan[:,:,0]\n",
    "    \n",
    "#     filename_tar = datapath + data[keyword][i]['target']\n",
    "#     itkimage_tar = sitk.ReadImage(filename_tar)\n",
    "#     target_scan = sitk.GetArrayFromImage(itkimage_tar)\n",
    "#     target_scan = cv2.resize(target_scan, (64, 64))\n",
    "#     if target_scan.ndim == 3 and target_scan.shape == (64, 64, 4):\n",
    "#         target_scan = target_scan[:,:,0]\n",
    "        \n",
    "#     # print(source_scan.shape, target_scan.shape)\n",
    "    \n",
    "#     # print(i, source_scan.min(), source_scan.max(), target_scan.min(), target_scan.max())\n",
    "    \n",
    "#     source_scan = (source_scan - np.min(source_scan)) / (np.max(source_scan) - np.min(source_scan))\n",
    "#     target_scan = (target_scan - np.min(target_scan)) / (np.max(target_scan) - np.min(target_scan))\n",
    "    \n",
    "#     pair = np.concatenate((source_scan[newaxis,:], target_scan[newaxis,:]), axis=0)\n",
    "#     outputs.append(pair)\n",
    "#     # print(i, source_scan.min(), source_scan.max(), target_scan.min(), target_scan.max())\n",
    "\n",
    "# test = torch.FloatTensor(outputs)#.unsqueeze(1)\n",
    "# print (test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced12f62-20c4-47e1-8317-0f336e5b5f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################Network optimization########################\n",
    "\n",
    "'''Check initilization'''\n",
    "from losses import MSE, Grad\n",
    "from network_epdiff import Unet\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "net = Unet(\n",
    "    in_shape = (xDim, yDim),\n",
    "    infeats = 2,\n",
    "    nb_features = [[16, 32, 32], [32, 32, 32, 16, 16]],\n",
    "    nb_levels = None,\n",
    "    max_pool = 2,\n",
    "    feat_mult = 1,\n",
    "    nb_conv_per_level = 1,\n",
    "    half_res = False,\n",
    "    skip_connection = True    \n",
    ")\n",
    "net = net.to(dev)\n",
    "\n",
    "class TDataset(Dataset):\n",
    "    def __init__(self, data, labels=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.labels[index] if self.labels is not None else None\n",
    "        return data, label\n",
    "\n",
    "\n",
    "train_set = TDataset(final_X_train, final_y_train)\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size = para.solver.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "test_set = TDataset(final_X_test, final_y_test)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size = para.solver.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "template_loss = 0\n",
    "printfreq = 1\n",
    "sigma = 0.02\n",
    "repara_trick = 0.0\n",
    "loss_array = torch.FloatTensor(para.solver.epochs,1).fill_(0)\n",
    "loss_array_val = torch.FloatTensor(para.solver.epochs,1).fill_(0)\n",
    "\n",
    "\n",
    "if(para.model.loss == 'L2'):\n",
    "    criterion = nn.MSELoss()\n",
    "elif (para.model.loss == 'L1'):\n",
    "    criterion = nn.L1Loss()\n",
    "if(para.model.optimizer == 'Adam'):\n",
    "    aug_optimizer = optim.Adam(net.parameters(), lr= para.solver.lr)\n",
    "elif (para.model.optimizer == 'SGD'):\n",
    "    aug_optimizer = optim.SGD(net.parameters(), lr= para.solver.lr, momentum=0.9)\n",
    "if (para.model.scheduler == 'CosAn'):\n",
    "    scheduler = CosineAnnealingLR(aug_optimizer, T_max=len(trainloader), eta_min=0)\n",
    "\n",
    "optimizer_template = optim.Adam(net.parameters(), lr= para.solver.lr)\n",
    "scheduler_template = CosineAnnealingLR(optimizer_template, T_max=len(trainloader), eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d302d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from network_epdiff import Unet\n",
    "# # in_shape = (48, 48)\n",
    "# in_shape = (64, 64)\n",
    "# # net_with_sc = Unet(\n",
    "# #     in_shape = in_shape,\n",
    "# #     infeats = 2,\n",
    "# #     nb_features = [[16, 32, 32], [32, 32, 32, 16, 16]],\n",
    "# #     nb_levels = None,\n",
    "# #     max_pool = 2,\n",
    "# #     feat_mult = 1,\n",
    "# #     nb_conv_per_level = 1,\n",
    "# #     half_res = False,\n",
    "# #     skip_connection = True    \n",
    "# # )\n",
    "# # trial_input = torch.randn(20, 2, in_shape[0], in_shape[1])\n",
    "# source = torch.randn(20, 1, in_shape[0], in_shape[1])\n",
    "# target = torch.randn(20, 1, in_shape[0], in_shape[1])\n",
    "# trial_input = torch.cat([source, target], dim=1).to(dev)\n",
    "# pred = net(trial_input)\n",
    "# print(pred[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds():\n",
    "    seed = random.randint(1, 100)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # if you're using CUDA\n",
    "    torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
    "\n",
    "# Define the model\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=9):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=56, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=56, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)  # Adjusted for input size 128x128\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 64 * 16 * 16)  # Adjusted for input size 128x128\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def classification(restart, times, clf_model, num_classes, aug_model, epochs, trainloader, testloader):\n",
    "    print('------------ Training Classifier ------------')\n",
    "\n",
    "    final_acc = []\n",
    "    final_prec = []\n",
    "    final_rec = []\n",
    "    final_f1 = []\n",
    "\n",
    "    finalw_acc = []\n",
    "    finalw_prec = []\n",
    "    finalw_rec = []\n",
    "    finalw_f1 = []\n",
    "    tr_loss = []\n",
    "\n",
    "    for run in range(0,restart,1):\n",
    "        set_random_seeds()\n",
    "\n",
    "        clf_optimizer = optim.Adam(clf_model.parameters(), lr= 0.00001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        print(\"Run: \", run)\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            print('----------------')\n",
    "            print('Epoch: ', epoch)\n",
    "            print('----------------')\n",
    "\n",
    "            total = 0\n",
    "            acc = 0\n",
    "            clf_model.train()\n",
    "            \n",
    "            ##### train with ground truth images #####\n",
    "            for j, image_data in enumerate(trainloader):\n",
    "                inputs, batch_labels = image_data\n",
    "                inputs = inputs.to(dev)\n",
    "                b, c, w, h = inputs.shape\n",
    "\n",
    "                src_bch = inputs[:,0,...].reshape(b,1,w,h)\n",
    "                tar_bch = inputs[:,1,...].reshape(b,1,w,h)\n",
    "\n",
    "                labels = [int(label) for label in batch_labels]\n",
    "                labels_tensor = torch.tensor(labels, dtype=torch.long).to(dev)\n",
    "\n",
    "                output = clf_model(tar_bch)\n",
    "                train_loss = criterion(output, labels_tensor)\n",
    "\n",
    "                clf_optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                clf_optimizer.step()\n",
    "\n",
    "                total += train_loss.item()\n",
    "                \n",
    "            ##### train with augmented images #####\n",
    "            for time in range(times):\n",
    "                for j, image_data in enumerate(trainloader):\n",
    "                    inputs, batch_labels = image_data\n",
    "                    inputs = inputs.to(dev)\n",
    "                    b, c, w, h = inputs.shape\n",
    "\n",
    "                    src_bch = inputs[:,0,...].reshape(b,1,w,h)\n",
    "                    tar_bch = inputs[:,1,...].reshape(b,1,w,h)\n",
    "\n",
    "                    aug_model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        x = torch.cat([src_bch, tar_bch], dim=1).to(dev)\n",
    "                        pred = aug_model(x)\n",
    "\n",
    "                    labels = [int(label) for label in batch_labels]\n",
    "                    labels_tensor = torch.tensor(labels, dtype=torch.long).to(dev)\n",
    "\n",
    "                    output = clf_model(pred[0])\n",
    "                    train_loss = criterion(output, labels_tensor)\n",
    "\n",
    "                    clf_optimizer.zero_grad()\n",
    "                    train_loss.backward()\n",
    "                    clf_optimizer.step()\n",
    "\n",
    "                    total += train_loss.item()\n",
    "\n",
    "            tr_loss.append(total)\n",
    "\n",
    "            print ('epoch:', epoch, 'total training loss:', total)#, 'tr accuracy: ')#, acc/(len(trainloader)*times), end=\"\\r\") \n",
    "\n",
    "            ### without test-time augmentation ###\n",
    "            print('------------------------------------------')\n",
    "            print('Without Test Time Augmentation Performance')\n",
    "            print('------------------------------------------')\n",
    "\n",
    "            acc = 0\n",
    "            predictions = []\n",
    "            targets = []\n",
    "            with torch.no_grad():\n",
    "                clf_model.eval()\n",
    "                aug_model.eval()\n",
    "                for j, image_data in enumerate(testloader):\n",
    "                    inputs, batch_labels = image_data\n",
    "                    inputs = inputs.to(dev)\n",
    "\n",
    "                    src_bch = inputs[:,0,...].reshape(b,1,w,h)\n",
    "                    tar_bch = inputs[:,1,...].reshape(b,1,w,h)\n",
    "\n",
    "                    outputs = clf_model(tar_bch)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                    labels = [int(label) for label in batch_labels]\n",
    "                    labels_tensor = torch.tensor(labels, dtype=torch.long).to(dev)\n",
    "\n",
    "                    predictions.extend(predicted.cpu().numpy())\n",
    "                    targets.extend(labels_tensor.cpu().numpy())\n",
    "\n",
    "                overall_accuracyw = accuracy_score(targets, predictions)\n",
    "                print(\"Testing Accuracy:\", overall_accuracyw)\n",
    "\n",
    "                precisionw = precision_score(targets, predictions, average='macro')\n",
    "                print(\"Precision:\", precisionw)\n",
    "\n",
    "                recallw = recall_score(targets, predictions, average='macro')\n",
    "                print(\"Recall:\", recallw)\n",
    "\n",
    "                f1w = f1_score(targets, predictions, average='macro')\n",
    "                print(\"F1-score:\", f1w)\n",
    "\n",
    "            ### with test-time augmentation ###\n",
    "            print('------------------------------------------')\n",
    "            print('With Test Time Augmentation Performance')\n",
    "            print('------------------------------------------')\n",
    "\n",
    "            acc = 0\n",
    "            predictions = []\n",
    "            targets = []\n",
    "            with torch.no_grad():\n",
    "                clf_model.eval()\n",
    "                aug_model.eval()\n",
    "                for j, image_data in enumerate(testloader):\n",
    "                    inputs, batch_labels = image_data\n",
    "                    inputs = inputs.to(dev)\n",
    "\n",
    "                    src_bch = inputs[:,0,...].reshape(b,1,w,h)\n",
    "                    tar_bch = inputs[:,1,...].reshape(b,1,w,h)\n",
    "\n",
    "                    outputs = clf_model(tar_bch)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                    labels = [int(label) for label in batch_labels]\n",
    "                    labels_tensor = torch.tensor(labels, dtype=torch.long).to(dev)\n",
    "\n",
    "                    correct = (predicted == labels_tensor).sum().item()\n",
    "                    accuracy = correct / labels_tensor.size(0) \n",
    "\n",
    "                    predictions.extend(predicted.cpu().numpy())\n",
    "                    targets.extend(labels_tensor.cpu().numpy())\n",
    "\n",
    "                for time in range(times):\n",
    "                    for j, image_data in enumerate(testloader):\n",
    "                        inputs, batch_labels = image_data\n",
    "                        inputs = inputs.to(dev)\n",
    "\n",
    "                        src_bch = inputs[:,0,...].reshape(b,1,w,h)\n",
    "                        tar_bch = inputs[:,1,...].reshape(b,1,w,h)\n",
    "\n",
    "                        x = torch.cat([src_bch, tar_bch], dim=1).to(dev)\n",
    "                        with torch.no_grad():\n",
    "                            pred = aug_model(x) \n",
    "\n",
    "                        outputs = clf_model(pred[0])\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                        labels = [int(label) for label in batch_labels]\n",
    "                        labels_tensor = torch.tensor(labels, dtype=torch.long).to(dev)\n",
    "\n",
    "                        correct = (predicted == labels_tensor).sum().item()\n",
    "                        accuracy = correct / labels_tensor.size(0) \n",
    "\n",
    "                        predictions.extend(predicted.cpu().numpy())\n",
    "                        targets.extend(labels_tensor.cpu().numpy())\n",
    "\n",
    "                overall_accuracy = accuracy_score(targets, predictions)\n",
    "                print(\"Testing Accuracy:\", overall_accuracy)\n",
    "\n",
    "                precision = precision_score(targets, predictions, average='macro')\n",
    "                print(\"Precision:\", precision)\n",
    "\n",
    "                recall = recall_score(targets, predictions, average='macro')\n",
    "                print(\"Recall:\", recall)\n",
    "\n",
    "                f1 = f1_score(targets, predictions, average='macro')\n",
    "                print(\"F1-score:\", f1)\n",
    "            \n",
    "        finalw_acc.append(overall_accuracyw)\n",
    "        finalw_prec.append(precisionw)\n",
    "        finalw_rec.append(recallw)\n",
    "        finalw_f1.append(f1w)\n",
    "            \n",
    "        final_acc.append(overall_accuracy)\n",
    "        final_prec.append(precision)\n",
    "        final_rec.append(recall)\n",
    "        final_f1.append(f1)\n",
    "\n",
    "    print(\"Number of images per class: \", num_images_per_digit)\n",
    "    print(\"Model: Conv; Restarts: \", restart, \"Times: \", times)\n",
    "    print('Acc with test-time aug: ', np.mean(final_acc), np.std(final_acc))\n",
    "    print('Acc without test-time aug: ', np.mean(finalw_acc), np.std(finalw_acc))\n",
    "    print('------------------------------------------------------')\n",
    "\n",
    "    print('Prec with test-time aug: ', np.mean(final_prec), np.std(final_prec))\n",
    "    print('Prec without test-time aug: ', np.mean(finalw_prec), np.std(finalw_prec))\n",
    "    print('------------------------------------------------------')\n",
    "\n",
    "    print('Rec with test-time aug: ', np.mean(final_rec), np.std(final_rec))\n",
    "    print('Rec without test-time aug: ', np.mean(finalw_rec), np.std(finalw_rec))\n",
    "    print('------------------------------------------------------')\n",
    "\n",
    "    print('F1-Sc with test-time aug: ', np.mean(final_f1), np.std(final_f1))\n",
    "    print('F1-Sc without test-time aug: ', np.mean(finalw_f1), np.std(finalw_f1))\n",
    "\n",
    "    return clf_model\n",
    "\n",
    "\n",
    "def MGAug(n_actual_epochs, trainloader, net, aug_optimizer, total_loss):\n",
    "    print('------------ Training MGAug ------------')\n",
    "    for epoch in range(n_actual_epochs): #min(para.solver.epochs, n_actual_epochs) n_actual_epochs\n",
    "        total= 0; \n",
    "        total_val = 0; \n",
    "        total_template = 0; \n",
    "        running_loss = 0\n",
    "        mse_loss = 0\n",
    "        reg_loss = 0\n",
    "        vae_loss = 0\n",
    "        net.train()\n",
    "        \n",
    "        for j, image_data in enumerate(trainloader):\n",
    "            inputs, batch_labels = image_data\n",
    "            inputs = inputs.to(dev)\n",
    "            b, c, w, h = inputs.shape\n",
    "            aug_optimizer.zero_grad()\n",
    "            src_bch = inputs[:,0,...].reshape(b,1,w,h)\n",
    "            tar_bch = inputs[:,1,...].reshape(b,1,w,h)\n",
    "            x = torch.cat([src_bch, tar_bch], dim=1).to(dev)\n",
    "            pred = net(x)  \n",
    "\n",
    "            Sdef = pred[0]\n",
    "            tar = tar_bch\n",
    "            recon_loss = torch.nn.MSELoss()(tar, Sdef)\n",
    "            regularization = (pred[2]*pred[3]).sum() / (tar.numel())\n",
    "            loss_total = recon_loss/(sigma*sigma) + 0.01*regularization + 0.001*pred[5]\n",
    "\n",
    "            loss_total.backward(retain_graph=True)\n",
    "            aug_optimizer.step()\n",
    "            running_loss += loss_total.item()\n",
    "            total += running_loss\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            mse_loss += recon_loss.item()\n",
    "            reg_loss += 0.01*regularization.item()\n",
    "            vae_loss += (0.001*pred[5].item())\n",
    "            \n",
    "        total_loss.append(total)\n",
    "        print ('epoch:', epoch, 'training loss:', total, 'mse_loss:', mse_loss, 'reg_loss:', reg_loss, 'kld_loss', vae_loss) #, end=\"\\r\"\n",
    "\n",
    "    return net, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85f3001-c839-4c8b-be3a-9716f1ae60a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ##################Training###################################\n",
    "########### Only Registration Training ###########\n",
    "\n",
    "n_actual_epochs = 100\n",
    "turns = 5\n",
    "times = 2\n",
    "num_classes = 10\n",
    "\n",
    "total_loss = []\n",
    "sigma = 0.1\n",
    "\n",
    "net = net.to(dev)\n",
    "clf_model = ConvNet(num_classes=num_classes).to(dev)\n",
    "\n",
    "restart = 1\n",
    "\n",
    "epoch_per_turns = int(n_actual_epochs/turns)\n",
    "\n",
    "for turn in range(0, turns, 1):\n",
    "    print('------------ Turn :', turn, '------------')\n",
    "    net, total_loss = MGAug(epoch_per_turns, trainloader, net, aug_optimizer, total_loss)\n",
    "    clf_model = classification(restart, times, clf_model, num_classes, net, epoch_per_turns, trainloader, testloader)\n",
    "    \n",
    "torch.save(net, './saved_models/joint_epdiff_mgaug_mnist_10p_2t.pth')\n",
    "torch.save(clf_model, './saved_models/joint_epdiff_mgaug_mnist_clf_10p_2t.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860fdc32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgaug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
