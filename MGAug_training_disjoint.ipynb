{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2cd1b6-69b3-41e0-83a7-a02f7b69fba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acc1db-7425-431e-b3f8-bcc830f43cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from typing import Dict, SupportsRound, Tuple, Any\n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch,gc\n",
    "from torch.autograd import grad\n",
    "from torch.autograd import Variable\n",
    "import torch.fft ############### Pytorch >= 1.8.0\n",
    "import torch.nn.functional as F\n",
    "import SimpleITK as sitk\n",
    "import os, glob\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "from PIL import Image\n",
    "import torch.nn.functional as nnf\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR,CosineAnnealingWarmRestarts,StepLR\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import random\n",
    "import yaml\n",
    "from tqdm import tqdm, trange\n",
    "from numpy import zeros, newaxis\n",
    "from easydict import EasyDict as edict\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import cv2\n",
    "import lagomorph as lm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda\"\n",
    "else:\n",
    "    dev = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21763858-5341-474c-b395-8a46bc6391cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################Parameter Loading#######################\n",
    "def read_yaml(path):\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            file = edict(yaml.load(f, Loader=yaml.FullLoader))\n",
    "        return file\n",
    "    except:\n",
    "        print('NO FILE READ!')\n",
    "        return None\n",
    "    \n",
    "para = read_yaml('./parameters.yml')\n",
    "\n",
    "xDim = para.data.x \n",
    "yDim = para.data.y\n",
    "zDim = para.data.z\n",
    "\n",
    "def loss_Reg(y_pred):\n",
    "        # For 3D reg\n",
    "        # dy = torch.abs(y_pred[:, :, 1:, :, :] - y_pred[:, :, :-1, :, :])\n",
    "        # dx = torch.abs(y_pred[:, :, :, 1:, :] - y_pred[:, :, :, :-1, :])\n",
    "        # dz = torch.abs(y_pred[:, :, :, :, 1:] - y_pred[:, :, :, :, :-1])\n",
    "        # dy = dy * dy\n",
    "        # dx = dx * dx\n",
    "        # dz = dz * dz\n",
    "        # d = torch.mean(dx) + torch.mean(dy) + torch.mean(dz)\n",
    "        # grad = d / 3.0\n",
    "\n",
    "        dy = torch.abs(y_pred[:, :, 1:, :] - y_pred[:, :, :-1, :])\n",
    "        dx = torch.abs(y_pred[:, :, :, 1:] - y_pred[:, :, :, :-1])\n",
    "\n",
    "        dy = dy * dy\n",
    "        dx = dx * dx\n",
    "        d = torch.mean(dx) + torch.mean(dy) \n",
    "        grad = d / 2.0\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30761975-46a0-4222-bd9d-1758cfcc7757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Load the dataset\n",
    "\n",
    "# Load the NumPy array from the pickle file\n",
    "with open('./datasets/mnist_10p_train_x.pkl', 'rb') as file:\n",
    "    final_X_train = pickle.load(file)\n",
    "\n",
    "with open('./datasets/mnist_10p_train_y.pkl', 'rb') as file:\n",
    "    final_y_train = pickle.load(file)\n",
    "\n",
    "with open('./datasets/mnist_10p_test_x.pkl', 'rb') as file:\n",
    "    final_X_test = pickle.load(file)\n",
    "\n",
    "with open('./datasets/mnist_10p_test_y.pkl', 'rb') as file:\n",
    "    final_y_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d060104-6824-42ed-93b9-877032e29369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ##################Training Data Loading##########################\n",
    "# readfilename = './2DShape/train_data' + '.json'\n",
    "# datapath = './2DShape/'\n",
    "# data = json.load(open(readfilename, 'r'))\n",
    "# outputs = []\n",
    "# keyword = 'train'\n",
    "# # outputs = np.array(outputs)\n",
    "\n",
    "# for i in trange (0,len(data[keyword])):\n",
    "#     filename_src = datapath + data[keyword][i]['source']\n",
    "#     itkimage_src = sitk.ReadImage(filename_src)\n",
    "#     source_scan = sitk.GetArrayFromImage(itkimage_src)\n",
    "#     source_scan = cv2.resize(source_scan, (64, 64))\n",
    "#     if source_scan.ndim == 3 and source_scan.shape == (64, 64, 4):\n",
    "#         source_scan = source_scan[:,:,0]\n",
    "    \n",
    "#     filename_tar = datapath + data[keyword][i]['target']\n",
    "#     itkimage_tar = sitk.ReadImage(filename_tar)\n",
    "#     target_scan = sitk.GetArrayFromImage(itkimage_tar)\n",
    "#     target_scan = cv2.resize(target_scan, (64, 64))\n",
    "#     if target_scan.ndim == 3 and target_scan.shape == (64, 64, 4):\n",
    "#         target_scan = target_scan[:,:,0]\n",
    "        \n",
    "#     # print(source_scan.shape, target_scan.shape)\n",
    "    \n",
    "#     # print(i, source_scan.min(), source_scan.max(), target_scan.min(), target_scan.max())\n",
    "    \n",
    "#     source_scan = (source_scan - np.min(source_scan)) / (np.max(source_scan) - np.min(source_scan))\n",
    "#     target_scan = (target_scan - np.min(target_scan)) / (np.max(target_scan) - np.min(target_scan))\n",
    "    \n",
    "#     pair = np.concatenate((source_scan[newaxis,:], target_scan[newaxis,:]), axis=0)\n",
    "#     outputs.append(pair)\n",
    "#     # print(pair.shape)\n",
    "#     # print(i, source_scan.min(), source_scan.max(), target_scan.min(), target_scan.max())\n",
    "\n",
    "# train = torch.FloatTensor(outputs)\n",
    "# print (train.shape)\n",
    "\n",
    "# ##################Testing Data Loading##########################\n",
    "# readfilename = './2DShape/test_data' + '.json'\n",
    "# datapath = './2DShape/'\n",
    "# data = json.load(open(readfilename, 'r'))\n",
    "# outputs = []\n",
    "# keyword = 'test'\n",
    "# # outputs = np.array(outputs)\n",
    "\n",
    "# for i in trange (0,len(data[keyword])):\n",
    "#     filename_src = datapath + data[keyword][i]['source']\n",
    "#     itkimage_src = sitk.ReadImage(filename_src)\n",
    "#     source_scan = sitk.GetArrayFromImage(itkimage_src)\n",
    "#     source_scan = cv2.resize(source_scan, (64, 64))\n",
    "#     if source_scan.ndim == 3 and source_scan.shape == (64, 64, 4):\n",
    "#         source_scan = source_scan[:,:,0]\n",
    "    \n",
    "#     filename_tar = datapath + data[keyword][i]['target']\n",
    "#     itkimage_tar = sitk.ReadImage(filename_tar)\n",
    "#     target_scan = sitk.GetArrayFromImage(itkimage_tar)\n",
    "#     target_scan = cv2.resize(target_scan, (64, 64))\n",
    "#     if target_scan.ndim == 3 and target_scan.shape == (64, 64, 4):\n",
    "#         target_scan = target_scan[:,:,0]\n",
    "        \n",
    "#     # print(source_scan.shape, target_scan.shape)\n",
    "    \n",
    "#     # print(i, source_scan.min(), source_scan.max(), target_scan.min(), target_scan.max())\n",
    "    \n",
    "#     source_scan = (source_scan - np.min(source_scan)) / (np.max(source_scan) - np.min(source_scan))\n",
    "#     target_scan = (target_scan - np.min(target_scan)) / (np.max(target_scan) - np.min(target_scan))\n",
    "    \n",
    "#     pair = np.concatenate((source_scan[newaxis,:], target_scan[newaxis,:]), axis=0)\n",
    "#     outputs.append(pair)\n",
    "#     # print(i, source_scan.min(), source_scan.max(), target_scan.min(), target_scan.max())\n",
    "\n",
    "# test = torch.FloatTensor(outputs)#.unsqueeze(1)\n",
    "# print (test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced12f62-20c4-47e1-8317-0f336e5b5f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################Network optimization########################\n",
    "\n",
    "'''Check initilization'''\n",
    "from losses import MSE, Grad\n",
    "from network_epdiff import Unet\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "net = Unet(\n",
    "    in_shape = (xDim, yDim),\n",
    "    infeats = 2,\n",
    "    nb_features = [[16, 32, 32], [32, 32, 32, 16, 16]],\n",
    "    nb_levels = None,\n",
    "    max_pool = 2,\n",
    "    feat_mult = 1,\n",
    "    nb_conv_per_level = 1,\n",
    "    half_res = False,\n",
    "    skip_connection = True    \n",
    ")\n",
    "\n",
    "\n",
    "class TDataset(Dataset):\n",
    "    def __init__(self, data, labels=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.labels[index] if self.labels is not None else None\n",
    "        return data, label\n",
    "\n",
    "\n",
    "\n",
    "train_set = TDataset(final_X_train, final_y_train)\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size = para.solver.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "test_set = TDataset(final_X_test, final_y_test)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size = para.solver.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "running_loss = 0 \n",
    "running_loss_val = 0\n",
    "template_loss = 0\n",
    "printfreq = 1\n",
    "sigma = 0.02\n",
    "repara_trick = 0.0\n",
    "loss_array = torch.FloatTensor(para.solver.epochs,1).fill_(0)\n",
    "loss_array_val = torch.FloatTensor(para.solver.epochs,1).fill_(0)\n",
    "\n",
    "\n",
    "if(para.model.loss == 'L2'):\n",
    "    criterion = nn.MSELoss()\n",
    "elif (para.model.loss == 'L1'):\n",
    "    criterion = nn.L1Loss()\n",
    "if(para.model.optimizer == 'Adam'):\n",
    "    optimizer = optim.Adam(net.parameters(), lr= para.solver.lr)\n",
    "elif (para.model.optimizer == 'SGD'):\n",
    "    optimizer = optim.SGD(net.parameters(), lr= para.solver.lr, momentum=0.9)\n",
    "if (para.model.scheduler == 'CosAn'):\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=len(trainloader), eta_min=0)\n",
    "\n",
    "optimizer_template = optim.Adam(net.parameters(), lr= para.solver.lr)\n",
    "scheduler_template = CosineAnnealingLR(optimizer_template, T_max=len(trainloader), eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85f3001-c839-4c8b-be3a-9716f1ae60a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ##################Training###################################\n",
    "########### Only MGAug Training ###########\n",
    "\n",
    "n_actual_epochs = 200\n",
    "total_loss = []\n",
    "sigma = 0.1\n",
    "net = net.to(dev)\n",
    "\n",
    "for epoch in range(n_actual_epochs): #min(para.solver.epochs, n_actual_epochs)\n",
    "    total= 0; \n",
    "    total_val = 0; \n",
    "    total_template = 0; \n",
    "    mse_loss = 0\n",
    "    total = 0\n",
    "    reg_loss = 0\n",
    "    vae_loss = 0\n",
    "    latent_f = []\n",
    "    net.train()\n",
    "    # print('epoch:', epoch)\n",
    "    all_labels = []\n",
    "    for j, image_data in enumerate(trainloader):\n",
    "        inputs, batch_labels = image_data\n",
    "        inputs = inputs.to(dev)\n",
    "        b, c, w, h = inputs.shape\n",
    "        optimizer.zero_grad()\n",
    "        src_bch = inputs[:,0,...].reshape(b,1,w,h)\n",
    "        tar_bch = inputs[:,1,...].reshape(b,1,w,h)\n",
    "        x = torch.cat([src_bch, tar_bch], dim=1).to(dev)\n",
    "        pred = net(x) #, registration = True     \n",
    "        # pred = net(src_bch, tar_bch, registration = True) #, registration = True     \n",
    "        # loss = criterion(pred[0], tar_bch) \n",
    "\n",
    "        Sdef = pred[0]\n",
    "        tar = tar_bch\n",
    "        recon_loss = torch.nn.MSELoss()(tar, Sdef)\n",
    "        regularization = (pred[2]*pred[3]).sum() / (tar.numel())\n",
    "        loss_total = recon_loss/(sigma*sigma) + 0.01*regularization + 0.001*pred[5]\n",
    "\n",
    "        loss_total.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        running_loss += loss_total.item()\n",
    "        total += running_loss\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        mse_loss += recon_loss.item()\n",
    "        reg_loss += 0.01*regularization.item()\n",
    "        vae_loss += (0.001*pred[5].item())\n",
    "        \n",
    "    total_loss.append(total)\n",
    "    print ('epoch:', epoch, 'training loss:', total, 'mse_loss:', mse_loss, 'reg_loss:', reg_loss, 'kld_loss', vae_loss, end=\"\\r\") \n",
    "    \n",
    "# torch.save(net, './saved_models/mgaug_lddmm_mnist_10p.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc82ad-5f11-4c5e-a23e-ac4f901b3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds():\n",
    "    seed = random.randint(1, 100)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # if you're using CUDA\n",
    "    torch.cuda.manual_seed_all(seed)  # For multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca48d7e-a9f1-4dda-9053-f6fd196bfe44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=9):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=56, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=56, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)  # Adjusted for input size 128x128\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 64 * 16 * 16)  # Adjusted for input size 128x128\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# input_data = torch.randn(10, 1, 64, 64).to(dev)\n",
    "# model = ConvNet(num_classes=10).to(dev)\n",
    "# output = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f4e9e-6ff9-4733-937c-9f5a8becf7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### Classification Training ########### \n",
    "\n",
    "times = 3\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "input_size = 64*64\n",
    "\n",
    "\n",
    "final_acc = []\n",
    "final_prec = []\n",
    "final_rec = []\n",
    "final_f1 = []\n",
    "\n",
    "finalw_acc = []\n",
    "finalw_prec = []\n",
    "finalw_rec = []\n",
    "finalw_f1 = []\n",
    "tr_loss = []\n",
    "\n",
    "restart = 5\n",
    "\n",
    "for run in range(0,restart,1):\n",
    "    set_random_seeds()\n",
    "    \n",
    "    model = ConvNet(num_classes=num_classes).to(dev)\n",
    "    clf_optimizer = optim.Adam(model.parameters(), lr= 0.00001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    aug_model = torch.load('./saved_models/mgaug_lddmm_mnist_10p.pth') \n",
    "    aug_model.eval()\n",
    "    \n",
    "    print(\"Run: \", run)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print('----------------')\n",
    "        print('Epoch: ', epoch)\n",
    "        print('----------------')\n",
    "\n",
    "        total = 0\n",
    "        acc = 0\n",
    "        model.train()\n",
    "        \n",
    "        ##### train with ground truth images #####\n",
    "        for j, image_data in enumerate(trainloader):\n",
    "            inputs, batch_labels = image_data\n",
    "            inputs = inputs.to(dev)\n",
    "            b, c, w, h = inputs.shape\n",
    "\n",
    "            src_bch = inputs[:,0,...].reshape(b,1,w,h)\n",
    "            tar_bch = inputs[:,1,...].reshape(b,1,w,h)\n",
    "\n",
    "            labels = [int(label) for label in batch_labels]\n",
    "            labels_tensor = torch.tensor(labels, dtype=torch.long).to(dev)\n",
    "\n",
    "            output = model(tar_bch)\n",
    "            train_loss = criterion(output, labels_tensor)\n",
    "\n",
    "            clf_optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            clf_optimizer.step()\n",
    "\n",
    "            total += train_loss.item()\n",
    "            \n",
    "        ##### train with augmented images #####\n",
    "        for time in range(times):\n",
    "            for j, image_data in enumerate(trainloader):\n",
    "                inputs, batch_labels = image_data\n",
    "                inputs = inputs.to(dev)\n",
    "                b, c, w, h = inputs.shape\n",
    "\n",
    "                src_bch = inputs[:,0,...].reshape(b,1,w,h)\n",
    "                tar_bch = inputs[:,1,...].reshape(b,1,w,h)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    x = torch.cat([src_bch, tar_bch], dim=1).to(dev)\n",
    "                    pred = aug_model(x)\n",
    "                    # pred = aug_model(src_bch, tar_bch, registration = True) \n",
    "\n",
    "                labels = [int(label) for label in batch_labels]\n",
    "                labels_tensor = torch.tensor(labels, dtype=torch.long).to(dev)\n",
    "\n",
    "                output = model(pred[0])\n",
    "                train_loss = criterion(output, labels_tensor)\n",
    "\n",
    "                clf_optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                clf_optimizer.step()\n",
    "\n",
    "                total += train_loss.item()\n",
    "\n",
    "        tr_loss.append(total)\n",
    "\n",
    "        print ('epoch:', epoch, 'total training loss:', total)#, 'tr accuracy: ')#, acc/(len(trainloader)*times), end=\"\\r\") \n",
    "\n",
    "        ### without test-time augmentation ###\n",
    "        print('------------------------------------------')\n",
    "        print('Without Test Time Augmentation Performance')\n",
    "        print('------------------------------------------')\n",
    "\n",
    "        acc = 0\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            aug_model.eval()\n",
    "            for j, image_data in enumerate(testloader):\n",
    "                inputs, batch_labels = image_data\n",
    "                inputs = inputs.to(dev)\n",
    "\n",
    "                src_bch = inputs[:,0,...].reshape(b,1,w,h)\n",
    "                tar_bch = inputs[:,1,...].reshape(b,1,w,h)\n",
    "\n",
    "                outputs = model(tar_bch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                labels = [int(label) for label in batch_labels]\n",
    "                labels_tensor = torch.tensor(labels, dtype=torch.long).to(dev)\n",
    "\n",
    "                predictions.extend(predicted.cpu().numpy())\n",
    "                targets.extend(labels_tensor.cpu().numpy())\n",
    "\n",
    "            overall_accuracyw = accuracy_score(targets, predictions)\n",
    "            print(\"Testing Accuracy:\", overall_accuracyw)\n",
    "\n",
    "            precisionw = precision_score(targets, predictions, average='macro')\n",
    "            print(\"Precision:\", precisionw)\n",
    "\n",
    "            recallw = recall_score(targets, predictions, average='macro')\n",
    "            print(\"Recall:\", recallw)\n",
    "\n",
    "            f1w = f1_score(targets, predictions, average='macro')\n",
    "            print(\"F1-score:\", f1w)\n",
    "\n",
    "        ### with test-time augmentation ###\n",
    "        print('------------------------------------------')\n",
    "        print('With Test Time Augmentation Performance')\n",
    "        print('------------------------------------------')\n",
    "\n",
    "        acc = 0\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            aug_model.eval()\n",
    "            for j, image_data in enumerate(testloader):\n",
    "                inputs, batch_labels = image_data\n",
    "                inputs = inputs.to(dev)\n",
    "\n",
    "                src_bch = inputs[:,0,...].reshape(b,1,w,h)\n",
    "                tar_bch = inputs[:,1,...].reshape(b,1,w,h)\n",
    "\n",
    "                outputs = model(tar_bch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                labels = [int(label) for label in batch_labels]\n",
    "                labels_tensor = torch.tensor(labels, dtype=torch.long).to(dev)\n",
    "\n",
    "                correct = (predicted == labels_tensor).sum().item()\n",
    "                accuracy = correct / labels_tensor.size(0) \n",
    "\n",
    "                predictions.extend(predicted.cpu().numpy())\n",
    "                targets.extend(labels_tensor.cpu().numpy())\n",
    "\n",
    "            for time in range(times):\n",
    "                for j, image_data in enumerate(testloader):\n",
    "                    inputs, batch_labels = image_data\n",
    "                    inputs = inputs.to(dev)\n",
    "\n",
    "                    src_bch = inputs[:,0,...].reshape(b,1,w,h)\n",
    "                    tar_bch = inputs[:,1,...].reshape(b,1,w,h)\n",
    "\n",
    "                    x = torch.cat([src_bch, tar_bch], dim=1).to(dev)\n",
    "                    pred = aug_model(x) \n",
    "\n",
    "                    outputs = model(pred[0])\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                    labels = [int(label) for label in batch_labels]\n",
    "                    labels_tensor = torch.tensor(labels, dtype=torch.long).to(dev)\n",
    "\n",
    "                    correct = (predicted == labels_tensor).sum().item()\n",
    "                    accuracy = correct / labels_tensor.size(0) \n",
    "\n",
    "                    predictions.extend(predicted.cpu().numpy())\n",
    "                    targets.extend(labels_tensor.cpu().numpy())\n",
    "\n",
    "            overall_accuracy = accuracy_score(targets, predictions)\n",
    "            print(\"Testing Accuracy:\", overall_accuracy)\n",
    "\n",
    "            precision = precision_score(targets, predictions, average='macro')\n",
    "            print(\"Precision:\", precision)\n",
    "\n",
    "            recall = recall_score(targets, predictions, average='macro')\n",
    "            print(\"Recall:\", recall)\n",
    "\n",
    "            f1 = f1_score(targets, predictions, average='macro')\n",
    "            print(\"F1-score:\", f1)\n",
    "           \n",
    "    finalw_acc.append(overall_accuracyw)\n",
    "    finalw_prec.append(precisionw)\n",
    "    finalw_rec.append(recallw)\n",
    "    finalw_f1.append(f1w)\n",
    "        \n",
    "    final_acc.append(overall_accuracy)\n",
    "    final_prec.append(precision)\n",
    "    final_rec.append(recall)\n",
    "    final_f1.append(f1)\n",
    "\n",
    "print(\"Model: Conv; Restarts: \", restart, \"Times: \", times)\n",
    "print('Acc with test-time aug: ', np.mean(final_acc), np.std(final_acc))\n",
    "print('Acc without test-time aug: ', np.mean(finalw_acc), np.std(finalw_acc))\n",
    "print('------------------------------------------------------')\n",
    "\n",
    "print('Prec with test-time aug: ', np.mean(final_prec), np.std(final_prec))\n",
    "print('Prec without test-time aug: ', np.mean(finalw_prec), np.std(finalw_prec))\n",
    "print('------------------------------------------------------')\n",
    "\n",
    "print('Rec with test-time aug: ', np.mean(final_rec), np.std(final_rec))\n",
    "print('Rec without test-time aug: ', np.mean(finalw_rec), np.std(finalw_rec))\n",
    "print('------------------------------------------------------')\n",
    "\n",
    "print('F1-Sc with test-time aug: ', np.mean(final_f1), np.std(final_f1))\n",
    "print('F1-Sc without test-time aug: ', np.mean(finalw_f1), np.std(finalw_f1))\n",
    "\n",
    "torch.save(model, './saved_models/mgaug_lddmm_clf_mnist_10p_3t.pth')\n",
    "torch.save(aug_model, './saved_models/mgaug_lddmm_mnist_10p.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f32cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgaug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
